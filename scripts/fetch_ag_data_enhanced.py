#!/usr/bin/env python3
"""
Enhanced Agricultural Data Fetcher for USDA NASS QuickStats API

NEW FEATURES:
- Handles "(D)" withheld values by summing disclosed subcategories
- Fetches all domain categories at once, then intelligently sums when needed
- Dramatically improves data completeness (68% -> 90%+ for livestock)

Previous features:
- Retry logic for failed API calls
- Detailed logging of all operations
- Data validation and completeness checks
- Intermediate caching to prevent data loss
- Progress tracking
"""

import requests
import pandas as pd
from functools import reduce
import time
import os
import json
import logging
from datetime import datetime
from typing import Dict, Optional, List, Tuple
from collections import defaultdict

# --- Setup Logging -----------------------------------------------------------

LOG_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

log_filename = os.path.join(LOG_DIR, f"fetch_ag_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_filename),
        logging.StreamHandler()  # Also print to console
    ]
)
logger = logging.getLogger(__name__)

# --- Constants ---------------------------------------------------------------

API_KEY = "E8A03C0D-5253-3598-B95D-C8B0D7F6F8B2"
BASE_URL = "https://quickstats.nass.usda.gov/api/api_GET/"

COMMON_PARAMS = {
    "key": API_KEY,
    "source_desc": "CENSUS",
    "year": "2022",
    "agg_level_desc": "COUNTY",
    "format": "JSON",
}

TARGET_STATES = ["OR", "WA", "CA", "NV", "ID", "MT"]

# API Configuration
MAX_RETRIES = 3
RETRY_DELAY = 5  # seconds
API_DELAY = 2    # seconds between requests

# --- Metrics Dictionary ------------------------------------------------------

METRICS = {
    # --- CORE LAND & OPERATIONS ---
    "farms": ("FARM OPERATIONS - NUMBER OF OPERATIONS", None),
    "cropland_acres": ("AG LAND, CROPLAND - ACRES", None),
    "harvested_cropland_acres": ("AG LAND, CROPLAND, HARVESTED - ACRES", None),
    "irrigated_acres": ("AG LAND, IRRIGATED - ACRES", {"prodn_practice_desc": "IRRIGATED"}),

    # --- FINANCIALS ---
    "market_value_total_dollars": ("COMMODITY TOTALS - SALES, MEASURED IN $", None),
    "crops_sales_dollars": ("CROP TOTALS - SALES, MEASURED IN $", None),
    "livestock_sales_dollars": ("ANIMAL TOTALS, INCL PRODUCTS - SALES, MEASURED IN $", None),
    "gov_payments_dollars": ("GOVT PROGRAMS, FEDERAL - RECEIPTS, MEASURED IN $", None),

    # --- CROP SPECIFICS ---
    "apples_acres": ("APPLES - ACRES BEARING & NON-BEARING", None),
    "wheat_acres": ("WHEAT - ACRES HARVESTED", None),
    "rice_acres": ("RICE - ACRES HARVESTED", None),
    "hazelnuts_acres": ("HAZELNUTS - ACRES BEARING & NON-BEARING", None),

    # Grass Seeds
    "grass_seed_bentgrass_acres": ("GRASSES, BENTGRASS, SEED - ACRES HARVESTED", None),
    "grass_seed_bermudagrass_acres": ("GRASSES, BERMUDA GRASS, SEED - ACRES HARVESTED", None),
    "grass_seed_bluegrass_acres": ("GRASSES, BLUEGRASS, KENTUCKY, SEED - ACRES HARVESTED", None),
    "grass_seed_bromegrass_acres": ("GRASSES, BROMEGRASS, SEED - ACRES HARVESTED", None),
    "grass_seed_fescue_acres": ("GRASSES, FESCUE, SEED - ACRES HARVESTED", None),
    "grass_seed_orchardgrass_acres": ("GRASSES, ORCHARDGRASS, SEED - ACRES HARVESTED", None),
    "grass_seed_ryegrass_acres": ("GRASSES, RYEGRASS, SEED - ACRES HARVESTED", None),
    "grass_seed_sudangrass_acres": ("GRASSES, SUDANGRASS, SEED - ACRES HARVESTED", None),
    "grass_seed_timothy_acres": ("GRASSES, TIMOTHY, SEED - ACRES HARVESTED", None),
    "grass_seed_wheatgrass_acres": ("GRASSES, WHEATGRASS, SEED - ACRES HARVESTED", None),

    # CORN
    "corn_acres": ("CORN, GRAIN - ACRES HARVESTED", None),
    "corn_silage_acres": ("CORN, SILAGE - ACRES HARVESTED", None),

    # HAY
    "hay_acres": ("HAY - ACRES HARVESTED", None),
    "haylage_acres": ("HAYLAGE - ACRES HARVESTED", None),

    # LIVESTOCK
    "beef_cattle_head": ("CATTLE, COWS, BEEF - INVENTORY", None),
    "dairy_cattle_head": ("CATTLE, COWS, MILK - INVENTORY", None),

    # Grass seed total
    "grass_seed_acres": ("GRASSES, SEED - ACRES HARVESTED", None),

    # --- FARM SIZE (For Small Tractor Potential) ---
    # Domain: AREA OPERATED
    "farms_1_9_acres": ("FARM OPERATIONS - NUMBER OF OPERATIONS", {"domain_desc": "AREA OPERATED", "domaincat_desc": "AREA OPERATED: (1.0 TO 9.9 ACRES)"}),
    "farms_10_49_acres": ("FARM OPERATIONS - NUMBER OF OPERATIONS", {"domain_desc": "AREA OPERATED", "domaincat_desc": "AREA OPERATED: (10.0 TO 49.9 ACRES)"}),
    "farms_50_69_acres": ("FARM OPERATIONS - NUMBER OF OPERATIONS", {"domain_desc": "AREA OPERATED", "domaincat_desc": "AREA OPERATED: (50.0 TO 69.9 ACRES)"}),
    "farms_70_99_acres": ("FARM OPERATIONS - NUMBER OF OPERATIONS", {"domain_desc": "AREA OPERATED", "domaincat_desc": "AREA OPERATED: (70.0 TO 99.9 ACRES)"}),
    "farms_100_139_acres": ("FARM OPERATIONS - NUMBER OF OPERATIONS", {"domain_desc": "AREA OPERATED", "domaincat_desc": "AREA OPERATED: (100 TO 139 ACRES)"}),
    "farms_140_179_acres": ("FARM OPERATIONS - NUMBER OF OPERATIONS", {"domain_desc": "AREA OPERATED", "domaincat_desc": "AREA OPERATED: (140 TO 179 ACRES)"}),
}

# --- Cache Directory ---------------------------------------------------------

CACHE_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "cache")
os.makedirs(CACHE_DIR, exist_ok=True)

# --- Helper Functions --------------------------------------------------------

def fetch_with_retry(url: str, params: dict, max_retries: int = MAX_RETRIES) -> Optional[dict]:
    """
    Fetch data from API with retry logic.
    Returns the JSON response or None if all retries failed.
    """
    for attempt in range(max_retries):
        try:
            logger.debug(f"API Request (attempt {attempt + 1}/{max_retries}): {params.get('short_desc', 'Unknown')} - {params.get('state_alpha', 'Unknown')}")

            r = requests.get(url, params=params, timeout=30)

            if r.status_code == 200:
                data = r.json()
                return data
            elif r.status_code == 429:  # Rate limit
                logger.warning(f"Rate limited. Waiting {RETRY_DELAY * 2} seconds...")
                time.sleep(RETRY_DELAY * 2)
            else:
                logger.warning(f"HTTP {r.status_code}: {r.text[:200]}")

        except requests.exceptions.Timeout:
            logger.warning(f"Timeout on attempt {attempt + 1}")
        except requests.exceptions.RequestException as e:
            logger.warning(f"Request error: {e}")
        except json.JSONDecodeError as e:
            logger.error(f"JSON decode error: {e}")

        if attempt < max_retries - 1:
            time.sleep(RETRY_DELAY)

    logger.error(f"All {max_retries} attempts failed for {params.get('short_desc', 'Unknown')}")
    return None


def process_county_records(records: List[dict]) -> Tuple[Optional[float], bool]:
    """
    Process all records for a single county and determine the best value.

    Logic:
    1. Look for "NOT SPECIFIED" (the aggregate total) - if not (D), use it
    2. If total is (D), sum up disclosed subcategories from a SINGLE domain to avoid double-counting
    3. Return (value, is_estimated_flag)

    Returns:
        (value, is_estimated): The value and whether it's estimated from subcategories
    """
    total_value = None
    total_is_withheld = False

    # Group subcategories by domain_desc to avoid double-counting
    domain_groups = defaultdict(list)

    for record in records:
        value_str = str(record.get("Value", "")).strip()
        domaincat = record.get("domaincat_desc", "")
        domain_desc = record.get("domain_desc", "")

        # Check for the aggregate total
        if domaincat == "NOT SPECIFIED":
            if value_str == "(D)" or not value_str:
                total_is_withheld = True
            else:
                # We have the actual total!
                try:
                    clean_value = value_str.replace(",", "")
                    total_value = float(clean_value)
                except (ValueError, TypeError):
                    pass

        # Collect subcategory values grouped by domain (skip withheld values)
        elif domaincat != "NOT SPECIFIED" and domain_desc != "TOTAL":
            if value_str != "(D)" and value_str:
                try:
                    clean_value = value_str.replace(",", "")
                    numeric_value = float(clean_value)
                    domain_groups[domain_desc].append(numeric_value)
                except (ValueError, TypeError):
                    pass

    # Decision logic
    if total_value is not None and not total_is_withheld:
        # We have an actual total, use it
        return (total_value, False)
    elif domain_groups:
        # Total is withheld - pick the domain with most complete data
        # Prefer domains like "INVENTORY OF BEEF COWS" over generic breakdowns
        best_domain = max(domain_groups.items(), key=lambda x: len(x[1]))
        return (sum(best_domain[1]), True)
    else:
        # No data at all
        return (None, False)


def fetch_metric_multistate(short_desc: str, metric_name: str, extra_params: dict = None) -> pd.DataFrame:
    """
    Fetches data for a specific metric across all TARGET_STATES with caching.

    Enhanced approach:
    - Fetches ALL domain categories at once (no domain_desc filter)
    - Groups records by county
    - Intelligently selects total vs. summed subcategories for each county
    """
    # Check cache first
    cache_file = os.path.join(CACHE_DIR, f"{metric_name}.csv")
    if os.path.exists(cache_file):
        logger.info(f"Loading {metric_name} from cache")
        return pd.read_csv(cache_file)

    all_counties_data = []
    total_estimated = 0
    total_actual = 0
    total_missing = 0

    for state in TARGET_STATES:
        params = COMMON_PARAMS.copy()
        params["short_desc"] = short_desc
        params["state_alpha"] = state
        # NOTE: We're NOT setting domain_desc - fetch ALL domains!

        if extra_params:
            params.update(extra_params)

        logger.info(f"Fetching {metric_name} for {state}...")

        data = fetch_with_retry(BASE_URL, params)

        if not data or "data" not in data or len(data["data"]) == 0:
            logger.warning(f"✗ {state}: No data available")
            continue

        records = data["data"]
        logger.info(f"  Retrieved {len(records)} records from API")

        # Group records by county
        county_records = defaultdict(list)
        for record in records:
            county = record.get("county_name", "")
            if county:
                county_records[county].append(record)

        # Process each county
        state_estimated = 0
        state_actual = 0
        state_missing = 0

        for county, county_data in county_records.items():
            value, is_estimated = process_county_records(county_data)

            if value is not None:
                all_counties_data.append({
                    "state_name": state,
                    "county_name": county,
                    "year": 2022,
                    "Value": value,
                    "is_estimated": is_estimated
                })

                if is_estimated:
                    state_estimated += 1
                    total_estimated += 1
                else:
                    state_actual += 1
                    total_actual += 1
            else:
                state_missing += 1
                total_missing += 1

        logger.info(f"✓ {state}: {state_actual + state_estimated} counties ({state_actual} actual, {state_estimated} estimated, {state_missing} missing)")
        time.sleep(API_DELAY)

    # Summary
    logger.info(f"Summary for {metric_name}:")
    logger.info(f"  ✓ Actual totals: {total_actual}")
    logger.info(f"  ≈ Estimated from subcategories: {total_estimated}")
    logger.info(f"  ✗ No data: {total_missing}")

    if not all_counties_data:
        logger.warning(f"No data collected for {metric_name}")
        return pd.DataFrame()

    combined_df = pd.DataFrame(all_counties_data)

    # Cache the result
    combined_df.to_csv(cache_file, index=False)
    logger.info(f"Cached {metric_name} ({len(combined_df)} total records)")

    return combined_df


def validate_data(df: pd.DataFrame, metric_name: str) -> Dict[str, any]:
    """
    Validate fetched data and return statistics.
    """
    stats = {
        'metric': metric_name,
        'total_rows': len(df),
        'unique_states': df['state_name'].nunique() if 'state_name' in df.columns else 0,
        'unique_counties': df['county_name'].nunique() if 'county_name' in df.columns else 0,
        'non_null_values': 0,
        'null_values': 0,
        'estimated_values': 0
    }

    if metric_name in df.columns:
        stats['non_null_values'] = df[metric_name].notna().sum()
        stats['null_values'] = df[metric_name].isna().sum()

    if f"{metric_name}_estimated" in df.columns:
        stats['estimated_values'] = df[f"{metric_name}_estimated"].sum()

    return stats


# --- Main Script Logic -------------------------------------------------------

def main():
    logger.info("=" * 80)
    logger.info("USDA NASS QuickStats Data Fetch - ENHANCED VERSION")
    logger.info("NEW: Handles (D) withheld values by summing disclosed subcategories!")
    logger.info(f"Target States: {', '.join(TARGET_STATES)}")
    logger.info(f"Total Metrics: {len(METRICS)}")
    logger.info(f"Log file: {log_filename}")
    logger.info("=" * 80)

    frames = []
    fetch_stats = []

    total_metrics = len(METRICS)

    for idx, (col_name, (desc, extras)) in enumerate(METRICS.items(), 1):
        logger.info(f"\n[{idx}/{total_metrics}] Fetching: {col_name}")
        logger.info(f"API Query: {desc}")

        try:
            df = fetch_metric_multistate(desc, col_name, extra_params=extras)

            if not df.empty:
                # Rename Value column to metric name
                df.rename(columns={"Value": col_name}, inplace=True)

                # Rename is_estimated to metric_estimated
                if "is_estimated" in df.columns:
                    df.rename(columns={"is_estimated": f"{col_name}_estimated"}, inplace=True)

                frames.append(df)

                stats = validate_data(df, col_name)
                fetch_stats.append(stats)

                logger.info(f"✓ {col_name} fetched successfully: {stats['total_rows']} rows ({stats.get('estimated_values', 0)} estimated)")
            else:
                logger.warning(f"✗ {col_name} returned no data")
                fetch_stats.append({'metric': col_name, 'total_rows': 0, 'error': 'No data'})

        except Exception as e:
            logger.error(f"✗ CRITICAL ERROR processing {col_name}: {e}", exc_info=True)
            fetch_stats.append({'metric': col_name, 'error': str(e)})

        logger.info("-" * 80)

    # --- Summary Report ---
    logger.info("\n" + "=" * 80)
    logger.info("FETCH SUMMARY")
    logger.info("=" * 80)

    successful = [s for s in fetch_stats if s.get('total_rows', 0) > 0]
    failed = [s for s in fetch_stats if s.get('total_rows', 0) == 0]

    logger.info(f"✓ Successfully fetched: {len(successful)}/{total_metrics} metrics")
    logger.info(f"✗ Failed/No data: {len(failed)}/{total_metrics} metrics")

    if failed:
        logger.warning("\nMetrics with issues:")
        for stat in failed:
            logger.warning(f"  - {stat['metric']}: {stat.get('error', 'No data available')}")

    if not frames:
        logger.error("\n❌ CRITICAL: No data was successfully fetched. Exiting.")
        return

    # --- Merge Data ---
    logger.info("\n" + "=" * 80)
    logger.info("MERGING DATA")
    logger.info("=" * 80)

    logger.info(f"Merging {len(frames)} dataframes...")

    merged = reduce(
        lambda left, right: pd.merge(
            left, right, on=["state_name", "county_name", "year"], how="outer"
        ),
        frames,
    )

    logger.info(f"✓ Merged successfully: {len(merged)} rows")

    # --- Data Cleaning ---
    logger.info("\n" + "=" * 80)
    logger.info("CLEANING DATA")
    logger.info("=" * 80)

    # Get all metric columns (excluding _estimated flag columns)
    metric_cols = [col for col in METRICS.keys() if col in merged.columns]

    logger.info(f"Cleaning {len(metric_cols)} columns...")

    for col in metric_cols:
        original_nulls = merged[col].isna().sum()

        # Ensure numeric type (should already be numeric from our fetch function)
        merged[col] = pd.to_numeric(merged[col], errors='coerce')

        new_nulls = merged[col].isna().sum()
        converted = new_nulls - original_nulls

        if converted > 0:
            logger.info(f"  {col}: {converted} values converted to NaN (invalid data)")

    # --- Data Completeness Report ---
    logger.info("\n" + "=" * 80)
    logger.info("DATA COMPLETENESS REPORT")
    logger.info("=" * 80)

    total_counties = len(merged)
    logger.info(f"Total counties in dataset: {total_counties}")
    logger.info("")

    for col in metric_cols:
        non_null = merged[col].notna().sum()
        pct = (non_null / total_counties) * 100

        # Check if we have estimated values
        estimated_col = f"{col}_estimated"
        estimated_count = 0
        if estimated_col in merged.columns:
            estimated_count = merged[estimated_col].sum()

        # Symbol
        if pct > 50:
            symbol = "✓"
        elif pct > 10:
            symbol = "⚠"
        else:
            symbol = "✗"

        est_str = f" ({estimated_count} estimated)" if estimated_count > 0 else ""
        logger.info(f"{symbol} {col:35s} : {non_null:3d}/{total_counties} ({pct:5.1f}%){est_str}")

    # --- Save Output ---
    logger.info("\n" + "=" * 80)
    logger.info("SAVING OUTPUT")
    logger.info("=" * 80)

    output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "public", "data")
    os.makedirs(output_dir, exist_ok=True)
    output_file = os.path.join(output_dir, "ag_data.csv")

    # Drop the _estimated flag columns from the final output (keep the data clean)
    estimated_cols = [col for col in merged.columns if col.endswith("_estimated")]
    output_df = merged.drop(columns=estimated_cols)

    output_df.to_csv(output_file, index=False)
    logger.info(f"✓ Saved to: {output_file}")
    logger.info(f"  Rows: {len(output_df)}")
    logger.info(f"  Columns: {len(output_df.columns)}")

    logger.info("\n" + "=" * 80)
    logger.info("✓ COMPLETE!")
    logger.info("=" * 80)


if __name__ == "__main__":
    main()
